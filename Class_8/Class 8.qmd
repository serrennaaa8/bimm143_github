---
title: "Class 8: Breast Cancer Analysis Mini Project"
author: "Serena Quezada (PID: A18556865)"
format: pdf
toc: true
---

## Background 

The goal of this mini-project is for you to explore a complete analysis using the unsupervised learning techniques covered in class.

The data itself comes from the Wisconsin Breast Cancer Diagnostic Data Set first reported by K. P. Benne and O. L. Mangasarian: “Robust Linear Programming Discrimination of Two Linearly Inseparable Sets”.

Values in this data set describe characteristics of the cell nuclei present in digitized images of a fine needle aspiration (FNA) of a breast mass.

## Data Import 

Data was downloaded from the class website as a CSV file 

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names = 1)
head(wisc.df)
```


## 1. Data Exploration 

The first column `diagnosis` is the expert opinion on the sample (i.e. patient FNA)

```{r}
wisc.data <- wisc.df[,-1]
dim(wisc.data)
```

Store the diagnosis as a vector for use later when we compare our results to those from experts in the field 

```{r}
diagnosis <- factor(wisc.df$diagnosis)
```

> Q1. How many observations are in this dataset?

There are 569 observations/patients in the dataset

>Q2. How many of the observations have a malignant diagnosis?

There are 212 observations that have a malignant diagnosis

```{r}
table(wisc.df$diagnosis)
```

> Q3. How many variables/features in the data are suffixed with _mean?

There are 10 variables/features in the data set that the "_mean" suffix 
```{r}
#colnames(wisc.data)
length( grep("_mean", colnames(wisc.data)) )
```


## 2. Principal Component Analyisis (PCA)

The `prcomp()` function to do PCA has a `scale=FALSE` default. In general we nearly always want to set this to TRUE so our analysis is not dominated by columns/variables in our dataset that have high standard deviation and mean when compared to others just because the unts of measurement are on different units/scales.


```{r}
wisc.pr <- prcomp(wisc.data, scale = TRUE)
summary(wisc.pr)
```

> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)? 

It is 0.4427

> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

The cumulative exceeds 0.70 after the third component (0.72636).

> Q6. How many principal components (PCs) are requires to describe at least 90% of the original variance in the data?

The cumulative proportion first passes the 0.90 threshold after the seventh component .91010. 



```{r}
biplot(wisc.pr, 
       xlabs = as.character(diagnosis),   
       cex = 0.6)
```

> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

It is difficult to understand because it has lots of plots and numbers bunched up together and you aren't able to read anything or see the plots.


Generating a more standard scatter plot 
```{r}
 plot(wisc.pr$x[,1:2],
      col = diagnosis,
      xlab = "PC1", ylab = "PC2")       
```

> Q8. Generate a similar plot for principal components  and 3. What do you notice about these plots?

```{r}
plot(wisc.pr$x[, c(1,3)], 
     col = diagnosis,
     xlab = "PC1", ylab = "PC3")
```

The first plot is very different from the 2nd and 3rd plot where in the 1st plot everything was bunched up together and plots couldn't be seen. In plot 2 and 3 it explains more variance, and is capturing a separation between the red and black samples properly.

```{r}
library(ggplot2)

ggplot(wisc.pr$x) +
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()

```

## PCA screeplot

A plot of how much variance each PC captures. We can get this from `wisc.pr$dev` or from the output of `summary(wisc.pr)`

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
pc.var <- wisc.pr$sdev^2

pve <- pc.var / sum(pc.var)
  plot(pve, xlab = "Principle Component", 
       ylab = "Proportion of Variance Explained",
       ylim = c(0,1), type = "o")
```

## Communicating PCA results

> Q9. For the first principal component, what is the component of the loading vector i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?

```{r}
wisc.pr$rotation["concave.points_mean", 1]
wisc.pr$rotation["concave.points_mean", "PC1"]
```

> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?

The minimum number is 5 principal components to explain 80% of the variance data, because PC5 exceeds 80% threshold.  


## 3. Hierarchical clustering

```{r}
data.scaled <- scale(wisc.data)

data.dist <- dist(data.scaled, method = "euclidean")

wisc.hclust <- hclust(data.dist, method = "complete")
```

```{r}
plot(wisc.hclust)
abline(h=150, col = "red", lty = 2)
h4 <- wisc.hclust$height[length(wisc.hclust$height) - 4 +1]
h4
clusters_at_h <- cutree(wisc.hclust, h=18.63658)
length(unique(clusters_at_h))
```

## 5. Combining methods (PCA and CLustering)

Clustering the original data was not very productive. The PCA results looked promising. Here we combine these methods by clustering from our PCA results. In other words"clustering in PC space"...

```{r}
grps <- cutree(wisc.hclust, k = 2)
table(grps)
```

```{r}
table(grps, diagnosis)
```
```{r}
plot(wisc.pr$x[, 1:2], col = grps)
```

```{r}
plot(wisc.pr$x[, 1:2], col = diagnosis)
```

```{r}
g <- as.factor(grps)
levels(g)
```
```{r}
g <- relevel(g,2)
levels(g)
```

> Q15. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
dist7 <- dist(wisc.pr$x[, 1:7], method = "euclidean")

wisc.pr.hclust <- hclust(dist7, method = "ward.D2")

wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k = 2)
table(wisc.pr.hclust.clusters, diagnosis)

```
The model does a good job of separating the two diagnoses. The model captures most cancers while keeping false-positive alerts for healthy patients relatively low. 


> Q16. How well do the k-means and hierarchical clustering models you created in previous sections do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses.

```{r}
wisc.df   <- read.csv("WisconsinCancer.csv", row.names = 1)
diagnosis <- factor(wisc.df$diagnosis)               # ensure it is a factor

wisc.data <- wisc.df[ , setdiff(names(wisc.df), "diagnosis")]

wisc.km <- kmeans(wisc.data, centers = 4)

dist.mat   <- dist(scale(wisc.data), method = "euclidean")
wisc.hclust <- hclust(dist.mat, method = "complete")
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)

table(wisc.km$cluster, diagnosis)            
table(wisc.hclust.clusters, diagnosis)           
    
```
 
The k-means model places each patient into one of the 4 clusters and each cluster is made up of a single diagnosis, ex. cluster 1 contains only malignant, cluster 2 contains benign and so on. This model is excellent at avoiding false-positive alerts which is good when you want to avoid unnecessary follow-up for benign patients. The hierarchical clustering model merges the 2 groups that cause the smallest increase in within-cluster variance. 

## 7. Prediction

```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```

```{r}

col.vec <- ifelse(diagnosis == "M", "red", "black")

plot(wisc.pr$x[, 1:2],           
     col = col.vec,              
     xlab = "PC 1", ylab = "PC 2",
     main = "PCA scores coloured by diagnosis")

points(npc[, 1], npc[, 2], col = "blue", pch = 16, cex = 3)

text(npc[, 1], npc[, 2], labels = c(1, 2), col = "white")
```
> Q18. Which of these new patients should we prioritize for follow up based on your results? 

The patients that fall into the clusters that are dominated by malignant diagnoses should be the first ones to schedule for a follow up. 

